{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa09f51-c7da-4a54-98ee-a50cbe1594ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Embedding,GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding, Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b0f702-1546-4315-9ff0-d53ae19ba3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6cc024-8435-4b06-95ce-5bb94e1fbbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Tools.NN as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31fedab4-775e-4975-b566-02fd2777d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./body')\n",
    "df_target = pd.read_csv('./target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6913addc-02ee-462c-a9ef-05a484b181f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df= df.merge(df_target, on = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee0a4a29-99fc-4a9a-8908-7d4d3e55294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df.drop('Unnamed: 0',axis =1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e28c29fc-c752-4751-868d-9f4a4acc2c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Looking for some authentic Japanese food at re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pepe Rosso is where you go when you're in SOHO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I had waited to return a couple other times to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This place is always busy - partly because it'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Love this place! I am not a regular yelper I d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  target\n",
       "0  Looking for some authentic Japanese food at re...       1\n",
       "1  Pepe Rosso is where you go when you're in SOHO...       1\n",
       "2  I had waited to return a couple other times to...       1\n",
       "3  This place is always busy - partly because it'...       1\n",
       "4  Love this place! I am not a regular yelper I d...       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d261b9f-0752-4acf-9176-708af4c47aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df['review'] = original_df['review'].apply(nn.clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa11743a-9f05-46a2-b08a-9a1710f42044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download if necessary\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c218a9-b85f-4115-acae-a3d640f09c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df['review'] = original_df['review'].apply(nn.remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b84dca2-ec62-464c-9710-72cd94f19184",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(original_df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97970321-4f7b-4986-8253-88db90977964",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features = 1000)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = original_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30df373e-74d1-4471-84ed-25a2af4952eb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'absolutely',\n",
       " 'accommodating',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addition',\n",
       " 'affordable',\n",
       " 'afternoon',\n",
       " 'ago',\n",
       " 'al',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'amazing',\n",
       " 'ambiance',\n",
       " 'ambience',\n",
       " 'american',\n",
       " 'amount',\n",
       " 'another',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'apparently',\n",
       " 'appetizer',\n",
       " 'appetizers',\n",
       " 'apple',\n",
       " 'area',\n",
       " 'around',\n",
       " 'arrived',\n",
       " 'artichoke',\n",
       " 'asian',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'ate',\n",
       " 'atmosphere',\n",
       " 'attention',\n",
       " 'attentive',\n",
       " 'authentic',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avocado',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'baked',\n",
       " 'balls',\n",
       " 'banana',\n",
       " 'banh',\n",
       " 'bar',\n",
       " 'barely',\n",
       " 'bartender',\n",
       " 'bartenders',\n",
       " 'based',\n",
       " 'basically',\n",
       " 'basil',\n",
       " 'bathroom',\n",
       " 'bbq',\n",
       " 'bean',\n",
       " 'beans',\n",
       " 'beat',\n",
       " 'beautiful',\n",
       " 'beef',\n",
       " 'beer',\n",
       " 'beers',\n",
       " 'behind',\n",
       " 'believe',\n",
       " 'belly',\n",
       " 'benedict',\n",
       " 'best',\n",
       " 'better',\n",
       " 'beyond',\n",
       " 'bf',\n",
       " 'big',\n",
       " 'bill',\n",
       " 'birthday',\n",
       " 'biscuit',\n",
       " 'bit',\n",
       " 'bite',\n",
       " 'bites',\n",
       " 'black',\n",
       " 'bland',\n",
       " 'block',\n",
       " 'bloody',\n",
       " 'blue',\n",
       " 'bone',\n",
       " 'bottle',\n",
       " 'bottom',\n",
       " 'bowl',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'bread',\n",
       " 'breakfast',\n",
       " 'bring',\n",
       " 'brisket',\n",
       " 'brooklyn',\n",
       " 'broth',\n",
       " 'brought',\n",
       " 'brunch',\n",
       " 'bucks',\n",
       " 'bun',\n",
       " 'bunch',\n",
       " 'buns',\n",
       " 'burger',\n",
       " 'burgers',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'butter',\n",
       " 'cafe',\n",
       " 'cake',\n",
       " 'calamari',\n",
       " 'call',\n",
       " 'called',\n",
       " 'came',\n",
       " 'cannot',\n",
       " 'care',\n",
       " 'cart',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'casual',\n",
       " 'certainly',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'cheese',\n",
       " 'cheesecake',\n",
       " 'chef',\n",
       " 'chewy',\n",
       " 'chicken',\n",
       " 'chili',\n",
       " 'chinatown',\n",
       " 'chinese',\n",
       " 'chips',\n",
       " 'chocolate',\n",
       " 'choice',\n",
       " 'choices',\n",
       " 'choose',\n",
       " 'city',\n",
       " 'clams',\n",
       " 'classic',\n",
       " 'clean',\n",
       " 'close',\n",
       " 'cocktail',\n",
       " 'cocktails',\n",
       " 'coconut',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'combination',\n",
       " 'combo',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'comfort',\n",
       " 'comfortable',\n",
       " 'coming',\n",
       " 'complaint',\n",
       " 'completely',\n",
       " 'complimentary',\n",
       " 'considering',\n",
       " 'conversation',\n",
       " 'cook',\n",
       " 'cooked',\n",
       " 'cooking',\n",
       " 'cool',\n",
       " 'corn',\n",
       " 'corner',\n",
       " 'cost',\n",
       " 'could',\n",
       " 'counter',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'cozy',\n",
       " 'crab',\n",
       " 'cramped',\n",
       " 'craving',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'creamy',\n",
       " 'crisp',\n",
       " 'crispy',\n",
       " 'crowd',\n",
       " 'crowded',\n",
       " 'crunchy',\n",
       " 'crust',\n",
       " 'cuban',\n",
       " 'cuisine',\n",
       " 'cup',\n",
       " 'curry',\n",
       " 'customer',\n",
       " 'customers',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'damn',\n",
       " 'dark',\n",
       " 'date',\n",
       " 'day',\n",
       " 'days',\n",
       " 'de',\n",
       " 'deal',\n",
       " 'decent',\n",
       " 'decided',\n",
       " 'decor',\n",
       " 'deep',\n",
       " 'def',\n",
       " 'definitely',\n",
       " 'delicious',\n",
       " 'delish',\n",
       " 'delivery',\n",
       " 'despite',\n",
       " 'dessert',\n",
       " 'desserts',\n",
       " 'die',\n",
       " 'different',\n",
       " 'diner',\n",
       " 'dining',\n",
       " 'dinner',\n",
       " 'dip',\n",
       " 'dipping',\n",
       " 'disappointed',\n",
       " 'disappointing',\n",
       " 'dish',\n",
       " 'dishes',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'door',\n",
       " 'dough',\n",
       " 'dressing',\n",
       " 'drink',\n",
       " 'drinking',\n",
       " 'drinks',\n",
       " 'dry',\n",
       " 'duck',\n",
       " 'due',\n",
       " 'dumpling',\n",
       " 'dumplings',\n",
       " 'early',\n",
       " 'easily',\n",
       " 'east',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eaten',\n",
       " 'eating',\n",
       " 'egg',\n",
       " 'eggplant',\n",
       " 'eggs',\n",
       " 'either',\n",
       " 'else',\n",
       " 'empanadas',\n",
       " 'empty',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enough',\n",
       " 'entire',\n",
       " 'entree',\n",
       " 'entrees',\n",
       " 'especially',\n",
       " 'establishment',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'evening',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'exactly',\n",
       " 'excellent',\n",
       " 'except',\n",
       " 'excited',\n",
       " 'expect',\n",
       " 'expectations',\n",
       " 'expected',\n",
       " 'expecting',\n",
       " 'expensive',\n",
       " 'experience',\n",
       " 'extra',\n",
       " 'extremely',\n",
       " 'fabulous',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'falafel',\n",
       " 'family',\n",
       " 'famous',\n",
       " 'fan',\n",
       " 'fancy',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'fat',\n",
       " 'fatty',\n",
       " 'favorite',\n",
       " 'favorites',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'feels',\n",
       " 'felt',\n",
       " 'filled',\n",
       " 'filling',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'first',\n",
       " 'fish',\n",
       " 'five',\n",
       " 'flavor',\n",
       " 'flavorful',\n",
       " 'flavors',\n",
       " 'fluffy',\n",
       " 'foods',\n",
       " 'forget',\n",
       " 'forgot',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'four',\n",
       " 'free',\n",
       " 'french',\n",
       " 'fresh',\n",
       " 'friday',\n",
       " 'fried',\n",
       " 'friend',\n",
       " 'friendly',\n",
       " 'friends',\n",
       " 'fries',\n",
       " 'front',\n",
       " 'fruit',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'garden',\n",
       " 'garlic',\n",
       " 'gave',\n",
       " 'gem',\n",
       " 'generous',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'ginger',\n",
       " 'girl',\n",
       " 'girlfriend',\n",
       " 'give',\n",
       " 'given',\n",
       " 'giving',\n",
       " 'glad',\n",
       " 'glass',\n",
       " 'glasses',\n",
       " 'gluten',\n",
       " 'gnocchi',\n",
       " 'go',\n",
       " 'goat',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'goodness',\n",
       " 'got',\n",
       " 'grab',\n",
       " 'greasy',\n",
       " 'greek',\n",
       " 'green',\n",
       " 'greens',\n",
       " 'grilled',\n",
       " 'grits',\n",
       " 'group',\n",
       " 'groups',\n",
       " 'guess',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'halal',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'hands',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hate',\n",
       " 'head',\n",
       " 'healthy',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'heaven',\n",
       " 'heavy',\n",
       " 'hell',\n",
       " 'help',\n",
       " 'helpful',\n",
       " 'high',\n",
       " 'highly',\n",
       " 'hit',\n",
       " 'hole',\n",
       " 'home',\n",
       " 'homemade',\n",
       " 'honestly',\n",
       " 'honey',\n",
       " 'hope',\n",
       " 'host',\n",
       " 'hostess',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'however',\n",
       " 'huge',\n",
       " 'hummus',\n",
       " 'hungry',\n",
       " 'husband',\n",
       " 'hype',\n",
       " 'ice',\n",
       " 'idea',\n",
       " 'immediately',\n",
       " 'impressed',\n",
       " 'included',\n",
       " 'including',\n",
       " 'incredible',\n",
       " 'incredibly',\n",
       " 'ingredients',\n",
       " 'inside',\n",
       " 'instead',\n",
       " 'interesting',\n",
       " 'interior',\n",
       " 'intimate',\n",
       " 'ippudo',\n",
       " 'italian',\n",
       " 'items',\n",
       " 'japanese',\n",
       " 'job',\n",
       " 'joint',\n",
       " 'juice',\n",
       " 'juicy',\n",
       " 'kale',\n",
       " 'keep',\n",
       " 'kept',\n",
       " 'kick',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'kitchen',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'known',\n",
       " 'korean',\n",
       " 'la',\n",
       " 'lamb',\n",
       " 'large',\n",
       " 'last',\n",
       " 'late',\n",
       " 'later',\n",
       " 'least',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'lemon',\n",
       " 'less',\n",
       " 'let',\n",
       " 'lettuce',\n",
       " 'level',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'limited',\n",
       " 'line',\n",
       " 'lines',\n",
       " 'list',\n",
       " 'literally',\n",
       " 'little',\n",
       " 'live',\n",
       " 'lived',\n",
       " 'lobster',\n",
       " 'local',\n",
       " 'location',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lot',\n",
       " 'lots',\n",
       " 'loud',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'lovely',\n",
       " 'low',\n",
       " 'lucky',\n",
       " 'lunch',\n",
       " 'mac',\n",
       " 'made',\n",
       " 'main',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'man',\n",
       " 'manager',\n",
       " 'mango',\n",
       " 'manhattan',\n",
       " 'many',\n",
       " 'market',\n",
       " 'matter',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'mayo',\n",
       " 'meal',\n",
       " 'meals',\n",
       " 'mean',\n",
       " 'means',\n",
       " 'meat',\n",
       " 'meatball',\n",
       " 'meatballs',\n",
       " 'meats',\n",
       " 'mediocre',\n",
       " 'medium',\n",
       " 'mention',\n",
       " 'mentioned',\n",
       " 'menu',\n",
       " 'mexican',\n",
       " 'mi',\n",
       " 'middle',\n",
       " 'might',\n",
       " 'milk',\n",
       " 'min',\n",
       " 'mind',\n",
       " 'mine',\n",
       " 'mins',\n",
       " 'minute',\n",
       " 'minutes',\n",
       " 'miso',\n",
       " 'miss',\n",
       " 'mix',\n",
       " 'mixed',\n",
       " 'modern',\n",
       " 'moist',\n",
       " 'mom',\n",
       " 'money',\n",
       " 'months',\n",
       " 'mood',\n",
       " 'morning',\n",
       " 'mostly',\n",
       " 'mouth',\n",
       " 'move',\n",
       " 'moved',\n",
       " 'mozzarella',\n",
       " 'much',\n",
       " 'mushroom',\n",
       " 'mushrooms',\n",
       " 'music',\n",
       " 'mussels',\n",
       " 'must',\n",
       " 'name',\n",
       " 'near',\n",
       " 'nearby',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'neighborhood',\n",
       " 'never',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'nicely',\n",
       " 'night',\n",
       " 'non',\n",
       " 'noodle',\n",
       " 'noodles',\n",
       " 'note',\n",
       " 'nothing',\n",
       " 'noticed',\n",
       " 'number',\n",
       " 'ny',\n",
       " 'nyc',\n",
       " 'octopus',\n",
       " 'offer',\n",
       " 'offered',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'oil',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'olive',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'onion',\n",
       " 'onions',\n",
       " 'open',\n",
       " 'opened',\n",
       " 'opinion',\n",
       " 'option',\n",
       " 'options',\n",
       " 'orange',\n",
       " 'order',\n",
       " 'ordered',\n",
       " 'ordering',\n",
       " 'orders',\n",
       " 'original',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'outdoor',\n",
       " 'outside',\n",
       " 'outstanding',\n",
       " 'oven',\n",
       " 'overall',\n",
       " 'overly',\n",
       " 'overpriced',\n",
       " 'owner',\n",
       " 'oysters',\n",
       " 'packed',\n",
       " 'pad',\n",
       " 'paid',\n",
       " 'pancake',\n",
       " 'pancakes',\n",
       " 'park',\n",
       " 'part',\n",
       " 'particularly',\n",
       " 'party',\n",
       " 'past',\n",
       " 'pasta',\n",
       " 'pay',\n",
       " 'people',\n",
       " 'pepper',\n",
       " 'peppers',\n",
       " 'per',\n",
       " 'perfect',\n",
       " 'perfection',\n",
       " 'perfectly',\n",
       " 'perhaps',\n",
       " 'person',\n",
       " 'pick',\n",
       " 'pickled',\n",
       " 'pickles',\n",
       " 'pie',\n",
       " 'piece',\n",
       " 'pieces',\n",
       " 'pig',\n",
       " 'pita',\n",
       " 'pizza',\n",
       " 'pizzas',\n",
       " 'places',\n",
       " 'plain',\n",
       " 'plan',\n",
       " 'plate',\n",
       " 'plates',\n",
       " 'platter',\n",
       " 'pleasant',\n",
       " 'please',\n",
       " 'plenty',\n",
       " 'plus',\n",
       " 'pm',\n",
       " 'poached',\n",
       " 'point',\n",
       " 'popular',\n",
       " 'pork',\n",
       " 'portion',\n",
       " 'portions',\n",
       " 'pot',\n",
       " 'potato',\n",
       " 'potatoes',\n",
       " 'prefer',\n",
       " 'prepared',\n",
       " 'pretty',\n",
       " 'price',\n",
       " 'priced',\n",
       " 'prices',\n",
       " 'pricey',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'pudding',\n",
       " 'pulled',\n",
       " 'put',\n",
       " 'quality',\n",
       " 'quick',\n",
       " 'quickly',\n",
       " 'quiet',\n",
       " 'quite',\n",
       " 'ramen',\n",
       " 'rare',\n",
       " 'rather',\n",
       " 'rating',\n",
       " 'ravioli',\n",
       " 'raw',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'reasonable',\n",
       " 'reasonably',\n",
       " 'recently',\n",
       " 'recommend',\n",
       " 'recommended',\n",
       " 'red',\n",
       " 'refreshing',\n",
       " 'regular',\n",
       " 'remember',\n",
       " 'reservation',\n",
       " 'reservations',\n",
       " 'rest',\n",
       " 'restaurant',\n",
       " 'restaurants',\n",
       " 'return',\n",
       " 'review',\n",
       " 'reviews',\n",
       " 'rib',\n",
       " 'ribs',\n",
       " 'rice',\n",
       " 'rich',\n",
       " 'ricotta',\n",
       " 'right',\n",
       " 'risotto',\n",
       " 'roast',\n",
       " 'roasted',\n",
       " 'roll',\n",
       " 'rolls',\n",
       " 'romantic',\n",
       " 'room',\n",
       " 'round',\n",
       " 'rude',\n",
       " 'run',\n",
       " 'rush',\n",
       " 'said',\n",
       " 'sake',\n",
       " 'salad',\n",
       " 'salads',\n",
       " 'salmon',\n",
       " 'salt',\n",
       " 'salty',\n",
       " 'sandwich',\n",
       " 'sandwiches',\n",
       " 'sangria',\n",
       " 'sat',\n",
       " 'satisfied',\n",
       " 'saturday',\n",
       " 'sauce',\n",
       " 'sauces',\n",
       " 'sausage',\n",
       " 'savory',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'scallops',\n",
       " 'sea',\n",
       " 'seafood',\n",
       " 'seasoned',\n",
       " 'seat',\n",
       " 'seated',\n",
       " 'seating',\n",
       " 'seats',\n",
       " 'second',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'selection',\n",
       " 'seriously',\n",
       " 'serve',\n",
       " 'served',\n",
       " 'server',\n",
       " 'servers',\n",
       " 'service',\n",
       " 'serving',\n",
       " 'sesame',\n",
       " 'set',\n",
       " 'several',\n",
       " 'share',\n",
       " 'shared',\n",
       " 'shop',\n",
       " 'short',\n",
       " 'show',\n",
       " 'shrimp',\n",
       " 'side',\n",
       " 'sides',\n",
       " 'sign',\n",
       " 'similar',\n",
       " 'simple',\n",
       " 'simply',\n",
       " 'since',\n",
       " 'single',\n",
       " 'sister',\n",
       " 'sit',\n",
       " 'sitting',\n",
       " 'size',\n",
       " 'sized',\n",
       " 'skin',\n",
       " 'slice',\n",
       " 'slices',\n",
       " 'sliders',\n",
       " 'slightly',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'smaller',\n",
       " 'smoked',\n",
       " 'soft',\n",
       " 'soggy',\n",
       " 'solid',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'sort',\n",
       " 'soup',\n",
       " 'sour',\n",
       " 'space',\n",
       " 'spaghetti',\n",
       " 'special',\n",
       " 'specials',\n",
       " 'spice',\n",
       " 'spicy',\n",
       " 'spinach',\n",
       " 'split',\n",
       " 'spot',\n",
       " 'spots',\n",
       " 'sprouts',\n",
       " 'square',\n",
       " 'st',\n",
       " 'staff',\n",
       " 'stand',\n",
       " 'standard',\n",
       " 'star',\n",
       " 'stars',\n",
       " 'start',\n",
       " 'started',\n",
       " 'stay',\n",
       " 'steak',\n",
       " 'steamed',\n",
       " 'stick',\n",
       " 'still',\n",
       " 'stop',\n",
       " 'stopped',\n",
       " 'store',\n",
       " 'street',\n",
       " 'strong',\n",
       " 'stuff',\n",
       " 'stuffed',\n",
       " 'style',\n",
       " 'sugar',\n",
       " 'suggest',\n",
       " 'summer',\n",
       " 'sunday',\n",
       " 'super',\n",
       " 'sure',\n",
       " 'surprise',\n",
       " 'surprised',\n",
       " 'surprisingly',\n",
       " 'sushi',\n",
       " 'sweet',\n",
       " 'table',\n",
       " 'tables',\n",
       " 'taco',\n",
       " 'tacos',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'takes',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tapas',\n",
       " 'taste',\n",
       " 'tasted',\n",
       " 'tastes',\n",
       " 'tasting',\n",
       " 'tasty',\n",
       " 'tea',\n",
       " 'tell',\n",
       " 'tender',\n",
       " 'terrible',\n",
       " 'texture',\n",
       " 'th',\n",
       " 'thai',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thick',\n",
       " 'thin',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'time',\n",
       " 'times',\n",
       " 'tiny',\n",
       " 'tip',\n",
       " 'toast',\n",
       " 'today',\n",
       " 'tofu',\n",
       " 'together',\n",
       " 'told',\n",
       " 'tomato',\n",
       " 'tomatoes',\n",
       " 'took',\n",
       " 'top',\n",
       " 'topped',\n",
       " 'toppings',\n",
       " 'total',\n",
       " 'totally',\n",
       " 'touch',\n",
       " 'town',\n",
       " 'traditional',\n",
       " 'treat',\n",
       " 'tried',\n",
       " 'trip',\n",
       " 'true',\n",
       " 'truffle',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'tuna',\n",
       " 'turned',\n",
       " 'twice',\n",
       " 'two',\n",
       " 'type',\n",
       " 'typical',\n",
       " 'understand',\n",
       " 'unfortunately',\n",
       " 'unique',\n",
       " 'unless',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'usually',\n",
       " 'value',\n",
       " 'variety',\n",
       " 'vegetable',\n",
       " 'vegetables',\n",
       " 'vegetarian',\n",
       " 'veggie',\n",
       " 'veggies',\n",
       " 'vibe',\n",
       " 'village',\n",
       " 'visit',\n",
       " 'visiting',\n",
       " 'waffles',\n",
       " 'wait',\n",
       " 'waited',\n",
       " 'waiter',\n",
       " 'waiters',\n",
       " 'waiting',\n",
       " 'waitress',\n",
       " 'walk',\n",
       " 'walked',\n",
       " 'walking',\n",
       " 'wall',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'warm',\n",
       " 'watch',\n",
       " 'water',\n",
       " 'way',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weeks',\n",
       " 'weird',\n",
       " 'well',\n",
       " 'went',\n",
       " 'west',\n",
       " 'whatever',\n",
       " 'white',\n",
       " 'whole',\n",
       " 'wife',\n",
       " 'window',\n",
       " 'wine',\n",
       " 'wines',\n",
       " 'wings',\n",
       " 'wish',\n",
       " 'within',\n",
       " 'without',\n",
       " 'wonderful',\n",
       " 'word',\n",
       " 'work',\n",
       " 'working',\n",
       " 'world',\n",
       " 'worth',\n",
       " 'would',\n",
       " 'wow',\n",
       " 'write',\n",
       " 'wrong',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yelp',\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'york',\n",
       " 'yum',\n",
       " 'yummy']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f30bc0b1-04dd-4e05-ba04-89a72d22f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer()\n",
    "X = tf_transformer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab32e846-5005-49d9-9e5a-4f87ddc71d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfVectorizer = TfidfVectorizer(max_features =1000)\n",
    "X = tfidfVectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c44260e-a0b8-4e82-9381-15daa75ee7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s, X_test_s , y_train_s, y_test_s = train_test_split(X, y, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9728adc-09f5-4ac4-9573-db12b9f8a24d",
   "metadata": {},
   "source": [
    "#### we take a look at the other classifiers and see how they perform. These numbers show that the models are doing well, however from the previous models, we see that these high scoring models nearly always predict true due to the skewness of the data. We need to confirm that the Neural net method is not the same as the numbers are fairly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12712a42-add5-4054-ab9d-df43d08b7a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GradientBoostingClassifier()  Accuracy  is 0.8982363389697758\n",
      "The GaussianNB()  Accuracy  is 0.6101903523126198\n",
      "The HistGradientBoostingClassifier()  Accuracy  is 0.8981803941313865\n",
      "The RandomForestClassifier()  Accuracy  is 0.8982083665505811\n",
      "The LogisticRegression()  Accuracy  is 0.8980545182450104\n"
     ]
    }
   ],
   "source": [
    "classifiers = [GradientBoostingClassifier(),GaussianNB(),HistGradientBoostingClassifier(),\n",
    "               RandomForestClassifier(),LogisticRegression()]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train_s,y_train_s)\n",
    "    print(f'The {classifier}  Accuracy  is {accuracy_score(y_test_s,classifier.predict(X_test_s)) }' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e26d4eb-b182-4d34-b716-43bd8bd70942",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = original_df['review']\n",
    "labels = original_df['target']\n",
    "X_train, X_test , y_train, y_test = train_test_split(docs, labels , random_state = 42, stratify = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "224c22ae-a469-4752-966d-f55074fbd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "\n",
    "X_train = [one_hot(d, vocab_size,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True, split=' ') for d in X_train]\n",
    "X_test = [one_hot(d, vocab_size,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True, split=' ') for d in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "486af7a8-dac6-40fa-b4e5-dfaea77d817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "851fdd99-2b20-467a-99e6-3c3d00b44d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential([\n",
    "    Embedding(vocab_size, 4, input_length=max_length),\n",
    "   Conv1D(128, 3, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "  Dense(5, activation='relu'),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e5a5002-9686-461e-a71d-a7d757e587ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20551e00-136a-4823-ac62-ae94864249a3",
   "metadata": {},
   "source": [
    "#### Running a more simplified veresion of the simple Neural Network model makes it a better fit. The training and test data shows 90% and 89% accuracy respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58a2c7c1-cfbc-4545-9468-bf3b316fcdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6703/6703 [==============================] - 70s 10ms/step - loss: 0.3251 - acc: 0.8989 - val_loss: 0.3074 - val_acc: 0.8982\n",
      "Epoch 2/20\n",
      "6703/6703 [==============================] - 70s 10ms/step - loss: 0.3050 - acc: 0.8977 - val_loss: 0.3068 - val_acc: 0.8982\n",
      "Epoch 3/20\n",
      "6703/6703 [==============================] - 72s 11ms/step - loss: 0.3017 - acc: 0.8972 - val_loss: 0.3059 - val_acc: 0.8982\n",
      "Epoch 4/20\n",
      "6703/6703 [==============================] - 72s 11ms/step - loss: 0.2926 - acc: 0.8981 - val_loss: 0.3074 - val_acc: 0.8981\n",
      "Epoch 5/20\n",
      "6703/6703 [==============================] - 73s 11ms/step - loss: 0.2882 - acc: 0.8978 - val_loss: 0.3091 - val_acc: 0.8981\n",
      "Epoch 6/20\n",
      "6703/6703 [==============================] - 72s 11ms/step - loss: 0.2820 - acc: 0.8988 - val_loss: 0.3160 - val_acc: 0.8980\n",
      "Epoch 7/20\n",
      "6703/6703 [==============================] - 72s 11ms/step - loss: 0.2790 - acc: 0.8988 - val_loss: 0.3213 - val_acc: 0.8956\n",
      "Epoch 8/20\n",
      "6703/6703 [==============================] - 71s 11ms/step - loss: 0.2727 - acc: 0.8988 - val_loss: 0.3238 - val_acc: 0.8957\n",
      "Epoch 9/20\n",
      "6703/6703 [==============================] - 72s 11ms/step - loss: 0.2686 - acc: 0.8990 - val_loss: 0.3347 - val_acc: 0.8924\n",
      "Epoch 10/20\n",
      "6703/6703 [==============================] - 72s 11ms/step - loss: 0.2624 - acc: 0.9016 - val_loss: 0.3298 - val_acc: 0.8957\n",
      "Epoch 11/20\n",
      "6703/6703 [==============================] - 76s 11ms/step - loss: 0.2628 - acc: 0.8996 - val_loss: 0.3417 - val_acc: 0.8964\n",
      "Epoch 12/20\n",
      "6703/6703 [==============================] - 73s 11ms/step - loss: 0.2586 - acc: 0.9003 - val_loss: 0.3416 - val_acc: 0.8952\n",
      "Epoch 13/20\n",
      "6703/6703 [==============================] - 67s 10ms/step - loss: 0.2559 - acc: 0.9013 - val_loss: 0.3494 - val_acc: 0.8964\n",
      "Epoch 14/20\n",
      "6703/6703 [==============================] - 66s 10ms/step - loss: 0.2549 - acc: 0.9010 - val_loss: 0.3464 - val_acc: 0.8891\n",
      "Epoch 15/20\n",
      "6703/6703 [==============================] - 66s 10ms/step - loss: 0.2504 - acc: 0.9032 - val_loss: 0.3484 - val_acc: 0.8928\n",
      "Epoch 16/20\n",
      "6703/6703 [==============================] - 65s 10ms/step - loss: 0.2495 - acc: 0.9024 - val_loss: 0.3531 - val_acc: 0.8848\n",
      "Epoch 17/20\n",
      "6703/6703 [==============================] - 65s 10ms/step - loss: 0.2502 - acc: 0.9025 - val_loss: 0.3559 - val_acc: 0.8874\n",
      "Epoch 18/20\n",
      "6703/6703 [==============================] - 66s 10ms/step - loss: 0.2474 - acc: 0.9035 - val_loss: 0.3596 - val_acc: 0.8776\n",
      "Epoch 19/20\n",
      "6703/6703 [==============================] - 66s 10ms/step - loss: 0.2425 - acc: 0.9049 - val_loss: 0.3570 - val_acc: 0.8841\n",
      "Epoch 20/20\n",
      "6703/6703 [==============================] - 66s 10ms/step - loss: 0.2446 - acc: 0.9034 - val_loss: 0.3664 - val_acc: 0.8902\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "505d63f2-6cb9-4b31-8a35-06b60ef668f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6703/6703 [==============================] - 26s 4ms/step - loss: 0.2351 - acc: 0.9047\n",
      "Training Accuracy is 90.47343730926514 \n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model1.evaluate(X_train,y_train)\n",
    "print('Training Accuracy is {} '.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79561e09-7821-4beb-9b56-76e82f571fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2235/2235 [==============================] - 9s 4ms/step - loss: 0.3664 - acc: 0.8902\n",
      "Testing Accuracy is 89.02222514152527 \n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model1.evaluate(X_test,y_test)\n",
    "print('Testing Accuracy is {} '.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b28e329-b440-427d-812a-9280787bf501",
   "metadata": {},
   "source": [
    "#### To test how well data is we need to create a series prediction based on the model. Using a confusion matrix we see that there is a good distribution where most of the reviews are classified as truths. This is true for both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38f5ef1c-c912-41fc-9f5b-43c8f1d10fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d94f751-6e79-4dfd-ae2a-aaa05769fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_train = [round(x[0]) for x in y_pred_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07b9fea8-11c5-46cf-be5e-e3df26679430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2521,  19309],\n",
       "       [  1125, 191540]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, rounded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "516e4a80-a3c8-416a-9a0c-3647be18bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b53f93e5-5877-4433-9036-46b5bbe00689",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded = [round(x[0]) for x in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1aa78065-7104-4096-9c9f-fb3a3e9bce9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  294,  6983],\n",
       "       [  866, 63356]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1460d4-1a58-420f-8801-ba0a0cefc58f",
   "metadata": {},
   "source": [
    "#### Based on the train and test data, the model seems to perform fairly well. A test needs to be conducted on the unseen data that was set aside. The data is cleaned and prepared the same way prior to analysis and we find that the model performed well with an 88.9% accuracy on the unseen data. This is only a slight improvement to the NB model which had a suprisingly good prediction for the unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c1e6b63-55a7-4a6f-a5c8-6f45b7401537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the unseen data and combine \n",
    "df2 = pd.read_csv('./Data/untouched_data')\n",
    "df_target2 = pd.read_csv('./Data/untouched_target')\n",
    "\n",
    "untouched_df = df2.merge(df_target2,\n",
    "                         how = 'inner',\n",
    "                         left_on = ['Unnamed: 0'],\n",
    "                         right_on = ['Unnamed: 0']\n",
    "                         )\n",
    "untouched_df.drop(['Unnamed: 0','user_id','prod_id','rating','date','name'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28c9f1a8-d7ac-418c-824f-6ec233fce9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Overall, great pizza, good service. My wife an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I thought I went to Japan when I got in.  The ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Holy Shmoly!! I just stood 90 minutes for a $2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a throw back restaurant in an old Ital...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a review by a vegetarian This is stiff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  target\n",
       "0  Overall, great pizza, good service. My wife an...       1\n",
       "1  I thought I went to Japan when I got in.  The ...       1\n",
       "2  Holy Shmoly!! I just stood 90 minutes for a $2...       1\n",
       "3  This is a throw back restaurant in an old Ital...       1\n",
       "4  This is a review by a vegetarian This is stiff...       1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untouched_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc1d3c01-7064-4c96-947e-99f6b41686e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_un,target_un = nn.NN_cleaning(untouched_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "921ba487-920f-44bc-a67e-bbbef3190d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2235/2235 [==============================] - 10s 4ms/step - loss: 0.3685 - acc: 0.8894\n",
      "Testing Accuracy is 88.93551230430603 \n"
     ]
    }
   ],
   "source": [
    "loss_un, accuracy_un = model1.evaluate(body_un,target_un)\n",
    "print('Testing Accuracy is {} '.format(accuracy_un*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "424d20ec-b96c-4299-8078-d4656f6a0ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_un = model1.predict(body_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b9ab520-3b98-4ce1-a7db-94d61d1cf2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_un = [round(x[0]) for x in pred_un]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5dbde81b-2b04-4886-bb2b-e5f160c7aaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  258,  7019],\n",
       "       [  892, 63330]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(target_un, rounded_un)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general-env",
   "language": "python",
   "name": "general-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
